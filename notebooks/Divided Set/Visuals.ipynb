{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cm0DI2mHwIZ"
      },
      "source": [
        "This notebook visualizes the metric, confusion metric and plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "zFt7owLK3W8q",
        "outputId": "a0f6c044-d89d-4c31-ebce-75ca10443a3d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, f1_score, jaccard_score,\n",
        "    accuracy_score, precision_score, recall_score\n",
        ")\n",
        "from tensorflow.keras.models import load_model\n",
        "import rasterio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from math import pi\n",
        "OUT_DIR = \"../experiments_final\"\n",
        "MODEL_DIR = OUT_DIR\n",
        "SAVE_DIR = os.path.join(OUT_DIR, \"evaluation_results\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "RAW_IMAGE = \"../data/raw/20241110_053942_45_24f7_3B_AnalyticMS_SR_8b_clip.tif\"\n",
        "RAW_MASK  = \"../data/raw/20241110_053942_45_24f7_3B_AnalyticMS_SR_8b_clip_Hybrid_mask.tif\"\n",
        "TEST_SAVE_X = os.path.join(SAVE_DIR, \"testX.npy\")\n",
        "TEST_SAVE_Y = os.path.join(SAVE_DIR, \"testY.npy\")\n",
        "def create_tiles(image, mask, tile_size=256):\n",
        "    tiles_img, tiles_mask = [], []\n",
        "    H, W, _ = image.shape\n",
        "    for i in range(0, H, tile_size):\n",
        "        for j in range(0, W, tile_size):\n",
        "            sub_img = image[i:i+tile_size, j:j+tile_size]\n",
        "            sub_mask = mask[i:i+tile_size, j:j+tile_size]\n",
        "            if sub_img.shape[:2] == (tile_size, tile_size):\n",
        "                tiles_img.append(sub_img)\n",
        "                tiles_mask.append(sub_mask)\n",
        "    tiles_img = np.array(tiles_img)\n",
        "    tiles_mask = np.array(tiles_mask)[..., np.newaxis] \n",
        "    return tiles_img, tiles_mask\n",
        "def load_test_tiles(img_path, mask_path, seed=42):\n",
        "    if os.path.exists(TEST_SAVE_X) and os.path.exists(TEST_SAVE_Y):\n",
        "        print(\"Loading pre-saved test tiles ...\")\n",
        "        testX = np.load(TEST_SAVE_X)\n",
        "        testY = np.load(TEST_SAVE_Y)\n",
        "        print(f\" Loaded test data: {testX.shape}, {testY.shape}\")\n",
        "        return testX, testY\n",
        "print(\" Rebuilding test tiles from image & mask ...\")\n",
        "    with rasterio.open(img_path) as s:\n",
        "        img = np.moveaxis(s.read(), 0, 2).astype(np.float32)\n",
        "        img = img / (np.max(img) + 1e-8)\n",
        "    with rasterio.open(mask_path) as s:\n",
        "        mask = s.read(1).astype(np.uint8)\n",
        "    X, Y = create_tiles(img, mask)\n",
        "    non_empty = np.sum(Y, axis=(1,2,3)) > 0\n",
        "    X, Y = X[non_empty], Y[non_empty]\n",
        "    _, tempX, _, tempY = train_test_split(X, Y, test_size=0.75, random_state=seed)\n",
        "    _, testX, _, testY = train_test_split(tempX, tempY, test_size=2/3, random_state=seed)\n",
        "    np.save(TEST_SAVE_X, testX)\n",
        "    np.save(TEST_SAVE_Y, testY)\n",
        "    print(f\" Test tiles rebuilt and saved ({testX.shape}, {testY.shape})\")\n",
        "    return testX, testY\n",
        "testX, testY = load_test_tiles(RAW_IMAGE, RAW_MASK)\n",
        "model_names = [\"unet\", \"resunet\", \"attnunet\", \"attnresunet\", \"asdms\"]\n",
        "models = {name: load_model(os.path.join(MODEL_DIR, f\"{name}.keras\"), compile=False)\n",
        "          for name in model_names}\n",
        "def evaluate_model(model, X, Y):\n",
        "    preds = (model.predict(X, verbose=0) > 0.5).astype(np.uint8)\n",
        "    y_true = Y.flatten()\n",
        "    y_pred = preds.flatten()\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    if cm.size == 4:\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "    else:\n",
        "        tn = fp = fn = tp = 0\n",
        "    metrics = {\n",
        "        \"IoU\": jaccard_score(y_true, y_pred, zero_division=0),\n",
        "        \"F1\": f1_score(y_true, y_pred, zero_division=0),\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
        "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
        "        \"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn\n",
        "    }\n",
        "    return metrics, cm\n",
        "all_metrics = {}\n",
        "conf_matrices = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n Evaluating {name} ...\")\n",
        "    metrics, cm = evaluate_model(model, testX, testY)\n",
        "    all_metrics[name] = metrics\n",
        "    conf_matrices[name] = cm\n",
        "    print(f\" {name.upper():10s} — F1={metrics['F1']:.4f}, IoU={metrics['IoU']:.4f}, Acc={metrics['Accuracy']:.4f}\")\n",
        "metrics_df = pd.DataFrame(all_metrics).T\n",
        "metrics_csv_path = os.path.join(SAVE_DIR, \"metrics_summary.csv\")\n",
        "metrics_df.to_csv(metrics_csv_path, index=True)\n",
        "print(f\"\\n Metrics saved to: {metrics_csv_path}\")\n",
        "for name, cm in conf_matrices.items():\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "                xticklabels=[\"Pred 0\",\"Pred 1\"], yticklabels=[\"True 0\",\"True 1\"])\n",
        "    plt.title(f\"Confusion Matrix — {name.upper()}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(SAVE_DIR, f\"{name}_confusion_matrix.png\"), dpi=300)\n",
        "    plt.close()\n",
        "    print(f\" Saved: {name}_confusion_matrix.png\")\n",
        "plt.figure(figsize=(10,6))\n",
        "metrics_df[[\"IoU\",\"F1\",\"Accuracy\",\"Precision\",\"Recall\"]].plot(kind='bar')\n",
        "plt.title(\" Model Performance Comparison (Landslide Segmentation)\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(SAVE_DIR, \"model_performance_comparison.png\"), dpi=300)\n",
        "plt.close()\n",
        "print(\"Saved bar chart comparison.\")\n",
        "categories = [\"IoU\", \"F1\", \"Accuracy\", \"Precision\", \"Recall\"]\n",
        "N = len(categories)\n",
        "angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
        "angles += angles[:1]\n",
        "plt.figure(figsize=(7,7))\n",
        "for name, row in metrics_df.iterrows():\n",
        "    values = row[categories].tolist()\n",
        "    values += values[:1]\n",
        "    plt.polar(angles, values, label=name)\n",
        "    plt.fill(angles, values, alpha=0.1)\n",
        "plt.title(\"Radar Chart — Model Metrics\", size=14)\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1.15))\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(SAVE_DIR, \"model_metrics_radar.png\"), dpi=300)\n",
        "plt.close()\n",
        "print(\"Saved radar chart of model metrics.\")\n",
        "print(\"\\n Evaluation Complete! Final Results:\")\n",
        "print(metrics_df.round(4))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
