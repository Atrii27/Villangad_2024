{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cm0DI2mHwIZ"
      },
      "source": [
        "This notebook visualizes the metric, confusion metric and plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaRPOln78NF4"
      },
      "outputs": [],
      "source": [
        "import os, numpy as np, matplotlib.pyplot as plt, rasterio\n",
        "from tensorflow.keras.models import load_model\n",
        "from rasterio.windows import Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp6_sa0j-5cj"
      },
      "outputs": [],
      "source": [
        "RAW_IMAGE = \"../data/raw/20241110_053942_45_24f7_3B_AnalyticMS_SR_8b_clip.tif\"\n",
        "RAW_MASK  = \"../data/raw/20241110_053942_45_24f7_3B_AnalyticMS_SR_8b_clip_Hybrid_mask.tif\"\n",
        "MODEL_DIR = \"../experiments_all\"\n",
        "SAVE_PATH = \"../experiments_all/full_comparison_scene.png\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35yFnuwh-8X-"
      },
      "outputs": [],
      "source": [
        "from rasterio.windows import Window\n",
        "def tile_raster_pair(image_path, mask_path, tile_size=IMG_SIZE):\n",
        "    imgs, msks = [], []\n",
        "    with rasterio.open(image_path) as img_src, rasterio.open(mask_path) as mask_src:\n",
        "        for top in range(0, img_src.height - tile_size[0] + 1, tile_size[0]):\n",
        "            for left in range(0, img_src.width - tile_size[1] + 1, tile_size[1]):\n",
        "                window = Window(left, top, tile_size[1], tile_size[0])\n",
        "                img = np.moveaxis(img_src.read(window=window), 0, 2).astype(np.float32)\n",
        "                mask = mask_src.read(1, window=window).astype(np.uint8)\n",
        "                img = img / (np.max(img) + 1e-8)\n",
        "                imgs.append(img)\n",
        "                msks.append(np.expand_dims(mask, -1))\n",
        "    return np.array(imgs), np.array(msks)\n",
        "X, Y = tile_raster_pair(RAW_IMAGE, RAW_MASK)\n",
        "print(f\" Loaded {len(X)} tiles for evaluation, shape {X.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEkIprw__VLI"
      },
      "outputs": [],
      "source": [
        "model_paths = {\n",
        "    \"U-Net\": os.path.join(OUT_DIR, \"unet.keras\"),\n",
        "    \"ResU-Net\": os.path.join(OUT_DIR, \"resunet.keras\"),\n",
        "    \"Attn-U-Net\": os.path.join(OUT_DIR, \"attnunet.keras\"),\n",
        "    \"Attn-ResU-Net\": os.path.join(OUT_DIR, \"attnresunet.keras\"),\n",
        "    \"ASDMS\": os.path.join(OUT_DIR, \"asdms.keras\")\n",
        "}\n",
        "models_loaded = {}\n",
        "for name, path in model_paths.items():\n",
        "    if os.path.exists(path):\n",
        "        models_loaded[name] = tf.keras.models.load_model(path, compile=False)\n",
        "        print(f\"Loaded {name}\")\n",
        "    else:\n",
        "        print(f\"Missing model file: {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DzCezg-_b66"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, X, Y_true):\n",
        "    preds = model.predict(X, verbose=0)\n",
        "    preds_bin = (preds > 0.5).astype(np.uint8)\n",
        "    y_true_flat = Y_true.flatten()\n",
        "    y_pred_flat = preds_bin.flatten()\n",
        "    acc = accuracy_score(y_true_flat, y_pred_flat)\n",
        "    pre = precision_score(y_true_flat, y_pred_flat, zero_division=0)\n",
        "    rec = recall_score(y_true_flat, y_pred_flat, zero_division=0)\n",
        "    f1 = f1_score(y_true_flat, y_pred_flat, zero_division=0)\n",
        "    iou = np.sum((y_true_flat & y_pred_flat)) / np.sum((y_true_flat | y_pred_flat) + 1e-6)\n",
        "    cm = confusion_matrix(y_true_flat, y_pred_flat)\n",
        "    return {\"Accuracy\": acc, \"Precision\": pre, \"Recall\": rec, \"F1\": f1, \"IoU\": iou}, preds_bin, cm\n",
        "metrics_dict, conf_matrices = {}, {}\n",
        "predictions = {}\n",
        "for name, model in models_loaded.items():\n",
        "    print(f\"\\n Evaluating {name}...\")\n",
        "    metrics, preds_bin, cm = evaluate_model(model, X, Y)\n",
        "    metrics_dict[name] = metrics\n",
        "    predictions[name] = preds_bin\n",
        "    conf_matrices[name] = cm\n",
        "    print(metrics)\n",
        "metrics_df = pd.DataFrame(metrics_dict).T\n",
        "metrics_df.to_csv(os.path.join(OUT_DIR, \"model_metrics.csv\"))\n",
        "print(\"\\n Metrics saved to model_metrics.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LpBiPDz_myK"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=metrics_df.index, y=metrics_df[\"Accuracy\"])\n",
        "plt.title(\"Model Accuracy Comparison\")\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"accuracy_graph.png\"), dpi=300)\n",
        "plt.show()\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=metrics_df.index, y=metrics_df[\"Recall\"])\n",
        "plt.title(\"Model Recall Comparison\")\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"recall_graph.png\"), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yK834Uf_qFW"
      },
      "outputs": [],
      "source": [
        "for name, cm in conf_matrices.items():\n",
        "    plt.figure(figsize=(4,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f\"Confusion Matrix â€” {name}\")\n",
        "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, f\"{name}_confusion_matrix.png\"), dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erA5XD0U_z22"
      },
      "outputs": [],
      "source": [
        "with rasterio.open(RAW_IMAGE) as src:\n",
        "    img = np.moveaxis(src.read(), 0, 2)\n",
        "    rgb = img[..., [4,2,1]] if img.shape[-1] > 4 else img[..., :3]\n",
        "    rgb = (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb) + 1e-6)\n",
        "with rasterio.open(RAW_MASK) as src:\n",
        "    mask = src.read(1)\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.imshow(rgb)\n",
        "plt.title(\"Satellite Image (Natural Color)\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(os.path.join(OUT_DIR, \"satellite_only.png\"), dpi=300)\n",
        "plt.show()\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.imshow(rgb, alpha=0.8)\n",
        "plt.imshow(mask, cmap=\"Reds\", alpha=0.5)\n",
        "plt.title(\"Satellite vs Mask Overlay\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig(os.path.join(OUT_DIR, \"satellite_vs_mask.png\"), dpi=300)\n",
        "plt.show()\n",
        "print(\"\\n All evaluations and visualizations completed!\")\n",
        "print(f\" Check results in folder: {OUT_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
